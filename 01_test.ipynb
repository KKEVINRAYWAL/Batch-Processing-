{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block is written in Python programming language using the PySpark library for processing data in a distributed computing environment. The code is intended to test if the PySpark and SparkSession are properly set up and configured to run batch processing jobs.\n",
    "\n",
    "Let's look at each part of the code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__file__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line of code imports the PySpark library, which is required to use Spark functionality in Python. The second line returns the location of the PySpark library file on the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code create a SparkSession object, which is the entry point for working with Spark. The .builder method is used to configure the SparkSession object. Here, we are specifying the master node to be run locally (local[*]), setting the application name to test, and creating a new SparkSession object if one does not exist already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head taxi+_zone_lookup.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines of code use the wget command to download a CSV file from a remote location and the head command to display the first few lines of the file in the output console. This is just an example of how to read a file from an external source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code read the CSV file taxi+_zone_lookup.csv into a DataFrame using the spark.read.csv() method. The option() method is used to specify that the file has a header row. The resulting DataFrame is then displayed using the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code write the DataFrame df to disk in the Parquet format using the df.write.parquet() method. The resulting files are stored in a directory called zones. The ls -lh command is used to display the size and permissions of the files in the current directory.\n",
    "\n",
    "Overall, this code block is useful for testing the PySpark and SparkSession setup before starting to work on batch processing jobs. This helps to ensure that the cluster is properly configured and the required libraries are installed before running larger processing jobs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
